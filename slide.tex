% Created 2023-06-19 Mon 12:52
% Intended LaTeX compiler: pdflatex
\documentclass[fleqn,aspectratio=1610]{beamer}
\author{Noboru Murata}
\date{\today}
\title{Change-Point Detection in a Sequence of Bags-of-Data}
\subtitle{an extension of anomaly analysis}
\usepackage[toc=none]{mytalk}
\addbibresource{papers.bib}
\graphicspath{{figs/},{refs/}}
\DeclareGraphicsExtensions{.pdf,.png,.eps,.jpg}
\institute{\url{https://noboru-murata.github.io/}}
\hypersetup{
 pdfauthor={Noboru Murata},
 pdftitle={Change-Point Detection in a Sequence of Bags-of-Data},
 pdfkeywords={time series analysis, change-point detection, machine learning},
 pdfsubject={based on Koshijima etal (2015), https://doi.org/10.1109/TKDE.2015.2426693},
 pdfcreator={Emacs 28.2 (Org mode 9.6.4)}, 
 pdflang={English}}
\begin{document}

\maketitle
\begin{frame}{Outline}
\tableofcontents
\end{frame}


\section{Introduction}
\label{sec:org86564c3}
\subsection{motivated examples}
\label{sec:org0741ec2}
\begin{frame}[label={sec:org44b06e5},t]{Anomaly \& Change-point Detection}
\begin{center}
\includegraphics[page=1,width=.8\linewidth]{changepoint_new}
\end{center}
\begin{itemize}
\item objective
\begin{itemize}
\item anomaly detection \\[0pt]
--- find an outlier of time series
\item \alert{change-point detection} \\[0pt]
--- find a drastic change of time series
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org9b34574},t]{Simple Stochastic Problem}
\begin{center}
\includegraphics[page=2,width=.8\linewidth]{changepoint_new}
\end{center}
\begin{overprint}
\onslide<2>
\begin{itemize}
\item generating mechanism
\begin{equation}
  X_{t}=
  \begin{cases}
    c_{0}+\varepsilon_{t},&t<t_{0},\\
    c_{1}+\varepsilon_{t},&t\geq t_{0},
  \end{cases}
  \quad \varepsilon_{t}\sim P
\end{equation}
\end{itemize}
\onslide<3>
\begin{itemize}
\item summary statistics
\begin{align}
  \bar{X}_{t}=
  &\frac{1}{\tau}\sum_{i=0}^{\tau-1}X_{t-i}\\
  &\text{estimates of mean values (moving average)}
\end{align}
\end{itemize}
\onslide<4>
\begin{center}
\includegraphics[page=3,width=.8\linewidth]{changepoint_new}      
\end{center}
\end{overprint}
\end{frame}

\begin{frame}[label={sec:org345c3d2},t]{Difficult Stochastic Problem}
\begin{center}
\includegraphics[page=4,width=.8\linewidth]{changepoint_new}
\end{center}
\begin{overprint}
\onslide<2>
\begin{itemize}
\item generating mechanism
\begin{equation}
  X_{t}=
  \begin{cases}
    c_{0}+\varepsilon_{t},&t<t_{0}, \quad \varepsilon_{t}\sim P\\
    c_{0}+\xi_{t},&t\geq t_{0}, \quad \xi_{t}\sim Q
  \end{cases}
\end{equation}
\end{itemize}
\onslide<3>
\begin{itemize}
\item summary statistics:
\begin{align}
  V_{t}=
  &\frac{1}{\tau'}\sum_{i=0}^{\tau'-1}(X_{t-i}-\bar{X}_{t})^{2}\\
  &\text{estimates of variances}
\end{align}
\end{itemize}
\onslide<4>
\begin{center}
\includegraphics[page=5,width=.8\linewidth]{changepoint_new}      
\end{center}
\end{overprint}
\end{frame}

\begin{frame}[label={sec:org93c3ced},t]{Slightly Difficult Problem}
\begin{center}
\includegraphics[page=11,width=.8\linewidth]{changepoint_new}
\end{center}
\begin{overprint}
\onslide<2>
\begin{itemize}
\item generating mechanism
\begin{equation}
  X_{t}=
  aX_{t-1}+bX_{t-2}+\varepsilon_{t},
  \quad\varepsilon_{t}\sim 
  \begin{cases}
    P,&t<t_{0}, \\
    Q,&t\geq t_{0}
  \end{cases}
\end{equation}
\end{itemize}
\onslide<3>
\begin{itemize}
\item summary statistics
\begin{align}
  Var(\hat{\varepsilon}_{t})
  &\quad
    \text{(estimated from \(X_{t},X_{t-1},\dotsc\))}\\
  &\text{estimates of innovation variances}\\
  &\hat{\varepsilon}_{t}=X_{t}-\hat{X}_{t}=X_{t}-(\hat{a}X_{t-1}+\hat{b}X_{t-2})
\end{align}
\end{itemize}
\onslide<4>
\begin{center}
\includegraphics[page=12,width=.8\linewidth]{changepoint_new}
\end{center}
\end{overprint}
\end{frame}

\begin{frame}[label={sec:orgd7ab62b},t]{More Difficult Problem}
\begin{center}
\includegraphics[page=9,width=.8\linewidth]{changepoint_new}
\end{center}
\begin{overprint}
\onslide<2>
\begin{itemize}
\item generating mechanism
\begin{equation}
  X_{t}=
  \begin{cases}
    a_{0}X_{t-1}+b_{0}X_{t-2}+\varepsilon_{t},&t<t_{0}, \\
    a_{1}X_{t-1}+b_{1}X_{t-2}+\varepsilon_{t},&t\geq t_{0},       
  \end{cases}
  \quad \varepsilon_{t}\sim P
\end{equation}
\end{itemize}
\onslide<3>
\begin{itemize}
\item summary statistics
\begin{align}
  \hat{a}_{t}, \hat{b}_{t}
  &\quad
    \text{(estimated from \(X_{t},X_{t-1},\dotsc\))}\\
  &\text{estimates of coefficients}
\end{align}
\end{itemize}
\begin{quote}
note: \alert{multi-dimensional problem}
\end{quote}
\onslide<4>
\begin{center}
\includegraphics[page=10,width=.8\linewidth]{changepoint_new}
\end{center}
\end{overprint}
\end{frame}

\subsection{target problem}
\label{sec:org88a2375}
\begin{frame}[label={sec:orgd088d70}]{Change-point Detection}
\begin{alertblock}{Problem}
find time points at which 
the generating mechanism of time series suddenly changes
\bigskip
\end{alertblock}
\begin{itemize}
\item applications
\begin{itemize}
\item intrusion detection in computer networks
\item irregular-motion detection in vision systems
\item signal segmentation in data stream
\item fraud detection in cellular systems
\item fault detection in engineering systems
\item etc.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orga82dbae}]{Standard Problem Setting}
\begin{itemize}
\item framework
\begin{itemize}
\item datum at time \(t\):  \(X_t\)\\[0pt]
a random variable (stochastic process)\\[0pt]
\alert{fixed length data vectors are considered}
\item objective \\[0pt]
examine whether \(X_t,X_{t+1},\dotsc\) differ from
\(X_{t-1},X_{t-2},\dotsc\) \\[4pt]
(or whether \%
\(X_t\) can be predicted from \(X_{t-1},X_{t-2},\dotsc\))
\item typical approach: define change-point scores, e.g.
\begin{equation}
  \mathrm{score}(X_t)=-\log\Pr(X_t|X_{t-1},X_{t-2},\dotsc)
\end{equation}
\alert{summary statistics are used for specifying probability models}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org34f2c28}]{Previous Works}
\begin{itemize}
\item representative algorithms
\begin{itemize}
\item Singular Spectrum Analysis \\[0pt]
(Moskvinaa \& Zhigljavskya, 2003) \nocite{MoskvinaZhigljavsky2003}
\item ChangeFinder \\[0pt]
(Takeuchi \& Yamanishi, 2006) \nocite{TakeuchiYamanishi2006}
\item Kullback-Leibler Importance Estimation Procedure \\[0pt]
(Sugiyama et al. 2007) \nocite{Sugiyama_etal2007nips}
\end{itemize}
\item differences of these approaches
\begin{itemize}
\item generative models of time series
\item computational costs
\item scalability of data size
\item sensitivity to change of regularity
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org40686f9},t]{Target Problem}
\begin{center}
\includegraphics[page=6,width=.75\linewidth]{changepoint_new}
\end{center}
\begin{overprint}
\onslide<2>
\begin{itemize}
\item generating mechanism
\begin{equation}
  X_{t}=
  \begin{cases}
    c_{0}+\varepsilon_{t},&t<t_{0}, \quad\varepsilon_{t}\sim P\\
    c_{0}+\xi_{t},&t\geq t_{0}, \quad\xi_{t}\sim Q
  \end{cases}
\end{equation}
\end{itemize}
\onslide<3>
\begin{itemize}
\item summary statistics
\begin{align}
  \bar{X}_{t}=&\frac{1}{\tau}\sum_{i=0}^{\tau-1}X_{t-i}
  &\text{(moving average)},\\
  % \bar{X}_{t}&\;\text{(moving average)},\\
  V_{t}=&\frac{1}{\tau'}\sum_{i=0}^{\tau'-1}(X_{t-i}-\bar{X}_{t})^{2}
               &\text{(volatility)}
  %V_{t}&\;\text{(volatility)}
\end{align}
\end{itemize}
\onslide<4>
\begin{center}
\includegraphics[page=7,width=.75\linewidth]{changepoint_new}
\end{center}
\onslide<5>
\begin{itemize}
\item summary statistics
\begin{align}
  \hat{P}_{t}
  &=\text{(density estimates of \(X_{t},X_{t-1},\dotsc\))}\\
  &\text{i.e. histogram, kernel density estimate, etc.}
\end{align}
\end{itemize}
\onslide<6>
\begin{center}
\includegraphics[page=8,width=.75\linewidth]{changepoint_new}
\end{center}
\end{overprint}
\end{frame}


\section{Problem Formulation}
\label{sec:org217cdcf}
\subsection{change-point in bags-of-data}
\label{sec:orgff26bea}
\begin{frame}[label={sec:org224b13b}]{Our Problem Setting}
\begin{itemize}
\item framework
\begin{itemize}
\item datum at time \(t\): \(B_t=\{X_i;i=1,\dotsc,n_t\}\) \\[0pt]
a set of random variables, i.e. a bag of data\\[0pt]
\alert{size of bag can be different in time}
\item objective: \\[0pt]
examine whether \(B_t,B_{t+1},\dotsc\) differ from 
\(B_{t-1},B_{t-2},\dotsc\) \\[4pt]
in statistical setup:\\[0pt]
\alert{examine whether \(\Pr(B_t)\) is predictable from \(\Pr(B_{t-1}),\Pr(B_{t-2}),\dotsc\)}

\nocite{KoshijimaHinoMurata2015}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgd0608a6}]{Sequence of Bags-of-Data}
\begin{center}
\includegraphics[width=\linewidth,viewport=0 20 960 340,clip]{time_series_hist}\\[0pt]
detect a change of distributions behind bags
\end{center}
\end{frame}

\begin{frame}[label={sec:org4a51d09}]{Problem setting}
\begin{itemize}
\item standard problem setting
\begin{center}
\includegraphics[width=.8\linewidth]{standard_problem_setting}
\end{center}
\item \alert{our problem setting}
\begin{center}
\includegraphics[width=.8\linewidth]{our_problem_setting}
\end{center}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org1b9b21a}]{Examples of Problem}
\begin{itemize}
\item graph-structured examples: sender-receiver scenario
\begin{itemize}
\item internet incident detection \\[0pt]
(relation between source and destination hosts)
\item Enron email dataset \\[0pt]
(relation between mail senders and receivers)
\item market trading analysis \\[0pt]
(relation between buyers and sellers)
\end{itemize}

\item other examples: multi-variate data
\begin{itemize}
\item multi-sensor plant data \\[0pt]
(colinearlity analysis of non-stationary data)
\item follow-up surveys \\[0pt]
(random missing)
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgc7354f3}]{Representation by Bags}
\begin{itemize}
\item parametric model
\begin{equation}
  B_t=\{X_{i}\}\sim P_{\theta_t}
\end{equation}
reduce to the change-point detection problem of
\(\{\theta_t\}\)
\item non-parametric model
\begin{equation}
  B_t=\{X_{i}\}\sim P_{B_t}
  \quad\text{(histogram, Parzen window, etc)}
  % \quad\text{(histogram, kernel density estimate, etc)}
\end{equation}
deal with probability distributions \(\{P_{B_t}\}\)
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org73bc095}]{Proposed Approach}
\begin{itemize}
\item non-parametric model: \alert{weighted data sets (histograms)}
\begin{itemize}
\item flexible for modeling various distributions
\item scalable for large sparse graphs
\end{itemize}
\end{itemize}
\bigskip
\begin{itemize}
\item twofold procedure for detection
\begin{itemize}
\item embed each \(P_{B_t}\) in an appropriate metric space
\item examine whether fluctuation of \(\{P_{B_t}\}\)
is anomalous or not
\end{itemize}
\end{itemize}
\end{frame}

\subsection{metric of bags-of-data}
\label{sec:org39dbb8a}
\begin{frame}[label={sec:org65be65e}]{Embedding in a Metric Space}
\begin{center}
\includegraphics[width=.8\linewidth]{embedding}\\[0pt]
detect a significant change by following a path of bags
\end{center}
\end{frame}

\begin{frame}[label={sec:org56d6828}]{Regular? or Anomalous?}
\begin{center}
\includegraphics[width=.5\linewidth]{our_problem_setting}
\\[10pt]
\includegraphics[width=.8\linewidth,viewport=50 0 814 360,clip]{metric_space}
\end{center}
\end{frame}

\begin{frame}[label={sec:orgf017324}]{Earth Mover's Distance}
\begin{itemize}
\item distance between distributions \(P\) and \(Q\):
\begin{itemize}
\item the least amount of work needed to match two distributions,
i.e. a kind of edit distance
\item proposed 
as a perceptually natural dissimilarity measure 
in computer vision
\item efficiently calculated by linear programming
\item mathematically equivalent to Wasserstein/Mallows distance
\end{itemize}
\end{itemize}

\begin{multline}
  D(P,Q)=\inf_{R} \mathbb{E}_{(X,Y\sim R)}[d(X,Y)],\text{ (\(d\) can be any distance)} \\
  \text{where } P(X)=\int R(X,dy),
  \text{ and } Q(Y)=\int R(dx,Y)
\end{multline}
\end{frame}

\begin{frame}[label={sec:org2742db4}]{Earth Mover's Distance}
\begin{center}
\includegraphics[width=.75\linewidth]{emd_final}
\\[10pt]
histogram: \(\{(\text{bin}, \text{freq})\}\);
\(P=\{(\boldsymbol{u},w)\}\), 
\(Q=\{(\boldsymbol{v},w')\}\)
\end{center}
\end{frame}

\againframe<1>{sec:org56d6828}

\subsection{two sample problem for bags-of-data}
\label{sec:orgc273c38}
\begin{frame}[label={sec:org017d3ef}]{Two Sample Problem}
\begin{alertblock}{Problem}
given i.i.d. observations 
\(\{x_{i};\,i=1,\dotsc,m\}\sim P\) and 
\(\{y_{j};\,j=1,\dotsc,n\}\sim Q\),
examine whether \(P\not=Q\)
\bigskip
\end{alertblock}
\begin{itemize}
\item possible criteria
\begin{itemize}
\item empirical mean (moment matching)
\item KL divergence with parametric models
\item KL divergence without models
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org58b9d4b}]{Information \& Entropy Estimators}
\begin{itemize}
\item distance-based entropy estimators

\nocite{HinoMurata2013}
\begin{itemize}
\item bags with weights: \(\mathfrak{D}=\{(B_i,w_i);i=1,\dotsc,n\}\)
\item information content
\begin{equation}
  I(B;\mathfrak{D})
  =c+d\sum_{B_i\in\mathfrak{D}}w_i\log D(B_i,B)
  \qquad(c,d:\;\text{const.})
\end{equation}
\item cross-entropy
\begin{equation}
  H(\mathfrak{D},\mathfrak{D}')
  =c+d\sum_{B_i\in\mathfrak{D},B'_j\in\mathfrak{D}'}
  w_iw'_j\log D(B_i,B'_j)
\end{equation}
\item auto-entropy
\begin{equation}
  H(\mathfrak{D})
  =c+d\sum_{B_i,B_j\in\mathfrak{D},B_j\not=B_i}
  \frac{w_iw_j}{1-w_i}\log D(B_i,B_j)
\end{equation}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orge181181}]{Change-Point Scores}
\begin{itemize}
\item reference and test datasets
\begin{align}
 \mathfrak{D}_{t}^{\mathrm{ref}}
 &=\{(B_i,w_i);i=t-1,t-2,\dotsc\}
  &&\text{ (past bags)}
 \\
 \mathfrak{D}_{t}^{\mathrm{test}}
 &=\{(B_i,w_i);i=t,t+1,\dotsc\}
  &&\text{ (future bags)}
\end{align}
where weights are used as discounting factors
\item likelihood ratio (f: density)
\begin{align}
 \mathrm{score}_{t}
 &=\log\frac{f_{\mathrm{test}}(B_t)}{f_{\mathrm{ref}}(B_t)}
 =I(B_t;\mathfrak{D}_{t}^{\mathrm{ref}})
 -I(B_t;\mathfrak{D}_{t}^{\mathrm{test}})
\end{align}
\item symmetric Kullback-Leibler divergence
\begin{align}
  \mathrm{score}_{t}
  % &=\frac{KL(\mathfrak{D}_{t}^{\mathrm{ref}}\|\mathfrak{D}_{t}^{\mathrm{test}})
  % +KL(\mathfrak{D}_{t}^{\mathrm{test}}\|\mathfrak{D}_{t}^{\mathrm{ref}})}{2}\\
  &=\frac{2H(\mathfrak{D}_{t}^{\mathrm{ref}},\mathfrak{D}_{t}^{\mathrm{test}})
   -H(\mathfrak{D}_{t}^{\mathrm{ref}})-H(\mathfrak{D}_{t}^{\mathrm{test}})}{2}
\end{align}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org2fd2478}]{Bootstrap Confidence Interval}
\begin{itemize}
\item Bayesian bootstrap: Bayesian analogue of the bootstrap
\begin{quote}
  instead of resampling from an empirical distribution,
  weighted samples are used where weights are sampled
  from the Dirichlet distribution
  \begin{align}
    (N_{1},\dotsc,N_{k})
    &\sim \mathrm{Mult}(n;\rho_{1},\dotsc,\rho_{k})
    &&\text{(resampling)}
    \\
    (W_{1},\dotsc,W_{k})
    &\sim \mathrm{Dir}(\alpha_{1},\dotsc,\alpha_{k})
    &&\text{(reweighting)}
  \end{align}
\end{quote}
\item if we let \(\alpha_{i}=n\rho_{i}\):
\begin{align}
  \mathbb{E}[N_{i}]&=\mathbb{E}[W_{i}]=\rho_{i}\\
  \mathrm{Var}[N_{i}]&=\mathrm{Var}[W_{i}]\cdot\frac{n+1}{n}
                     =\frac{\rho_{i}(1-\rho_{i})}{n}
\end{align}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org0466614}]{Anomaly Detection}
\begin{itemize}
\item confidence interval with Baysian bootstrap on weights of bags
\begin{itemize}
\item \alert{regular:} intervals intersect each other
\item \alert{anomalous:} otherwise
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=.6\linewidth]{hypo_testing}
\end{center}
\end{frame}


\section{Numerical Examples}
\label{sec:org082c2f3}
\subsection{enron corpus analysis}
\label{sec:orgfe13fe5}
\begin{frame}[label={sec:orgde3a921}]{ENRON Corpus}
\begin{exampleblock}{Enron Email Dataset (Cohen, 2009)}\label{sec:org070c4f2}
email transmission data from about 150 users,
mostly senior management of Enron
\end{exampleblock}
\begin{itemize}
\item duration: 2000/6 -- 2002/5 (accounting scandal: 2001)
\item time window size of bags: 1 week
\item size of reference datasets: 5 weeks
\item size of test datasets: 3 weeks
\item statistics in bags: 7 stats of bipartite graphs
\begin{itemize}
\item degree of sender / receiver
\item 2nd order degree of sender-sender / receiver-receiver
\item number of messages from sender / to receiver
\item number of messages between sender and receiver
\end{itemize}
\item confidence interval: 0.95
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org0eabc0f}]{Results of Analysis}
\begin{center}
\includegraphics[width=.8\linewidth]{enronBig}
\end{center}
\nocite{Sun_etal2007kdd}
\end{frame}


\section{Conclusion}
\label{sec:org4a7f2de}
\begin{frame}[label={sec:orgcdb8128}]{Concluding Remarks}
we consider
\begin{itemize}
\item change-point detection for sequence of bags of data
\item a statistically appropriate distance between bags-of-data
\item change-point scores based on entropy estimators
\item confidence intervals with Bayesian bootstrap
\end{itemize}
possible extension would be
\begin{itemize}
\item on-line detection with stable entropy estimators
\item on-line adaptive thresholding
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{References}
\printbibliography[heading=none]
\end{frame}
\end{document}